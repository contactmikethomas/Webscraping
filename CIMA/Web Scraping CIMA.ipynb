{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CIMA Webscraping Firms</h1>\n",
    "\n",
    "This jupyter notebook contains the code used to webscrape CIMA Firms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importing webscraping libraries</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Defining the Webscraping Spider</h2>\n",
    "\n",
    "This code block is defining what our web spider will do - note that it isn't running it, just defining it. See that we are extending the exsiting scrapy.Spider class rather than doing everything from scratch, so we only have minimal coding to do.\n",
    "\n",
    "We tested this code using one page:\n",
    "\n",
    "https://www.cimaglobal.com/About-us/Find-a-CIMA-Accountant/?p=3&surname=&company=&city=&county=&country=United+Kingdom&funcspecialism=&sectorspecialism=&sortby=country&results=500#Results\n",
    "\n",
    "CIMA are helpful enough to return as many search results as you want, but the real details are kept on the page for each accountant, so we can easily scrape the links but we will have to ping each page to get member details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "class MySpider(scrapy.Spider):\n",
    "    name = \"CIMA\"\n",
    "\n",
    "    custom_settings = {\n",
    "        'DOWNLOAD_DELAY': 1,\n",
    "    }\n",
    "    \n",
    "    def start_requests(self):\n",
    "        \n",
    "        urls = []\n",
    "        \n",
    "        for i in range(1,4):\n",
    "            url = 'https://www.cimaglobal.com/About-us/Find-a-CIMA-Accountant/?p='+str(i)+'&surname=&company=&city=&county=&country=United+Kingdom&funcspecialism=&sectorspecialism=&sortby=country&results=500#Results'\n",
    "            urls.append(url)\n",
    "        \n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "            \n",
    "    def parse(self, response):\n",
    "        f_links = response.xpath('//ul[@class=\"accountantListing\"]//p[@class=\"accountantListing-button\"]//a//@href').extract()\n",
    "\n",
    "        firm_links = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_links]\n",
    "                        \n",
    "        full_lk_output.append(firm_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Running the Webscraping</h2>\n",
    "\n",
    "Note, you can't re run the code below in a single session for one reason or another, so you need to restart the kernel between runs.\n",
    "\n",
    "This code creates a lightweight container for our webspider and then runs it - to be honest understanding this is probably optional unless it breaks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "full_lk_output = []\n",
    "\n",
    "process = CrawlerProcess()\n",
    "process.crawl(MySpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now downloaded all the pages that we want to scrape. The first thing to do is to examine what we got back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(full_lk_output[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final_lk_output = []\n",
    "\n",
    "for i in full_lk_output:\n",
    "    for j in i:\n",
    "        final_lk_output.append(j.encode('ascii','replace'))\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle.dump(final_lk_output, open('/home/de-admin/Documents/Webscraping/CIMA_links.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got all the links, let's get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.cimaglobal.com/About-us/Find-a-CIMA-Accountant/Blue-Osprey-Limited-10507/', 'http://www.cimaglobal.com/About-us/Find-a-CIMA-Accountant/Tina-Murray-9869/']\n",
      "1473\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "CIMA_Links = pickle.load(open('/home/de-admin/Documents/Webscraping/CIMA_links.p','rb'))\n",
    "\n",
    "print(str(CIMA_Links[0:2]))\n",
    "print(len(CIMA_Links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MySpider(scrapy.Spider):\n",
    "    name = \"CIMA\"\n",
    "\n",
    "    #custom_settings = {\n",
    "    #    \"RETRY_TIMES\" : 10,\n",
    "    #    \"RETRY_HTTP_CODES\":[500, 503, 504, 400, 403, 404, 408],\n",
    "    #    \"DOWNLOADER_MIDDLEWARES\":{'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,'scrapy_proxies.RandomProxy': 100,'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110},\n",
    "    #    \"PROXY_LIST\":'/home/de-admin/Documents/Webscraping/proxy_list.txt',\n",
    "    #    \"PROXY_MODE\": 0\n",
    "    #}\n",
    "    \n",
    "    custom_settings = {\n",
    "        'DOWNLOAD_DELAY': 0.5,\n",
    "    }\n",
    "    \n",
    "    def start_requests(self):\n",
    "\n",
    "        for url in CIMA_Links[30:32]:\n",
    "            yield scrapy.Request(url=str(url), callback=self.parse)\n",
    "            \n",
    "    def parse(self, response):\n",
    "        f_details = response.xpath('//div[@class=\"searchResultDetails-details wysiwyg\"]//dt//text() | //div[@class=\"searchResultDetails-details wysiwyg\"]//dd//text()').extract()\n",
    "        f_specs = response.xpath('//div[@class=\"searchResultDetails-details wysiwyg\"]//h3//text() | //div[@class=\"searchResultDetails-details wysiwyg\"]//li//text()').extract()\n",
    "                      \n",
    "        full_sp_output.append(f_specs)\n",
    "        full_dt_output.append(f_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-29 17:21:36 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\n",
      "2017-09-29 17:21:36 [scrapy.utils.log] INFO: Overridden settings: {}\n",
      "2017-09-29 17:21:36 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-09-29 17:21:36 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-09-29 17:21:36 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-09-29 17:21:36 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-09-29 17:21:36 [scrapy.core.engine] INFO: Spider opened\n",
      "2017-09-29 17:21:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-09-29 17:21:36 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2017-09-29 17:21:36 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.cimaglobal.com/About-us/Find-a-CIMA-Accountant/Wahid-Ahmed--Co-Ltd-11280/> from <GET http://www.cimaglobal.com/About-us/Find-a-CIMA-Accountant/Wahid-Ahmed--Co-Ltd-11280/>\n",
      "2017-09-29 17:21:37 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.cimaglobal.com/About-us/Find-a-CIMA-Accountant/DAVID-AKAKPO-13134/> from <GET http://www.cimaglobal.com/About-us/Find-a-CIMA-Accountant/DAVID-AKAKPO-13134/>\n",
      "2017-09-29 17:21:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.cimaglobal.com/About-us/Find-a-CIMA-Accountant/Wahid-Ahmed--Co-Ltd-11280/> (referer: None)\n",
      "2017-09-29 17:21:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.cimaglobal.com/About-us/Find-a-CIMA-Accountant/DAVID-AKAKPO-13134/> (referer: None)\n",
      "2017-09-29 17:21:38 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2017-09-29 17:21:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 1338,\n",
      " 'downloader/request_count': 4,\n",
      " 'downloader/request_method_count/GET': 4,\n",
      " 'downloader/response_bytes': 41682,\n",
      " 'downloader/response_count': 4,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'downloader/response_status_count/302': 2,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 9, 29, 16, 21, 38, 840499),\n",
      " 'log_count/DEBUG': 5,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 4,\n",
      " 'scheduler/dequeued/memory': 4,\n",
      " 'scheduler/enqueued': 4,\n",
      " 'scheduler/enqueued/memory': 4,\n",
      " 'start_time': datetime.datetime(2017, 9, 29, 16, 21, 36, 403172)}\n",
      "2017-09-29 17:21:38 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "full_sp_output = []\n",
    "full_dt_output = []\n",
    "\n",
    "process = CrawlerProcess()\n",
    "process.crawl(MySpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'Functional specialisms', u'Book-keeping/accounting', u'Business planning/budgeting', u'Business process improvement', u'Business tax planning/advice', u'Cash flow management/treasury', u'Company secretarial matters', u'Contribution/profit analysis', u'Cost reduction', u'Costing/accounting systems', u'Interim management', u'Internal audit/risk analysis', u'Management performance reports', u'Payroll/NI/PAYE administration', u'Personal tax planning/advice', u'Sector specialisms', u'Transportation/automotive', u'Retail services', u'Manufacturing', u'Hotel/catering/travel', u'Food/drink', u'Distribution'], [u'Functional specialisms', u'Book-keeping/accounting', u'Business planning/budgeting', u'Business process improvement', u'Business tax planning/advice', u'Contribution/profit analysis', u'Cost reduction', u'Costing/accounting systems', u'Management performance reports', u'Payroll/NI/PAYE administration', u'Personal tax planning/advice']]\n",
      "[[u'Company Name:', u'Wahid Ahmed & Co Ltd', u'\\xa0', u'Surname:', u'Ahmed', u'\\xa0', u'First Name:', u'Wahiduzzaman', u'\\xa0', u'Mobile Phone:', u'\\xa0', u'Work Phone:', u'0207 3751512', u'\\xa0', u'Email Address:', u'\\r\\n\\t\\t\\t                            ', u'wahidahmed2009@hotmail.com', u'\\xa0\\r\\n\\t\\t\\t                        ', u'Homepage:', u'\\xa0', u'Address line 1:', u'Wahid Ahmed & CoLtd', u'\\xa0', u'Address line 2:', u'67 Brick Lane, 1st Floor', u'\\xa0', u'Address line 3:', u'\\xa0', u'Postal Code:', u'E1 6QL', u'\\xa0', u'City:', u'LONDON', u'\\xa0', u'County', u'\\xa0', u'Country:', u'United Kingdom', u'\\xa0', u'Size of practice:', u'1 - 10', u'\\xa0'], [u'Company Name:', u'DAVID AKAKPO', u'\\xa0', u'Surname:', u'Akakpo', u'\\xa0', u'First Name:', u'David', u'\\xa0', u'Mobile Phone:', u'+4407788242264', u'\\xa0', u'Work Phone:', u'01908671516', u'\\xa0', u'Email Address:', u'\\r\\n\\t\\t\\t                            ', u'david.akakpo@boxileconsulting.com', u'\\xa0\\r\\n\\t\\t\\t                        ', u'Homepage:', u'\\xa0', u'Address line 1:', u'61 PENRYN AVENUE', u'\\xa0', u'Address line 2:', u'FISHERMEAD', u'\\xa0', u'Address line 3:', u'\\xa0', u'Postal Code:', u'MK6 2BG', u'\\xa0', u'City:', u'MILTON KEYNES', u'\\xa0', u'County', u'BUCKS', u'\\xa0', u'Country:', u'United Kingdom', u'\\xa0', u'Size of practice:', u'1 - 10', u'\\xa0']]\n"
     ]
    }
   ],
   "source": [
    "print(full_sp_output)\n",
    "print(full_dt_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_index_ad = []\n",
    "row_index_co = []\n",
    "row_index_wb = []\n",
    "row_index_em = []\n",
    "\n",
    "for i in range(len(full_nm_output)):\n",
    "    ad = []\n",
    "    for j in full_nm_output[i]:\n",
    "        if j.encode('ascii','replace') in full_ad_output[i]:\n",
    "            ad.append(full_ad_output[i].index(j.encode('ascii','replace')))\n",
    "    row_index_ad.append(ad)\n",
    "\n",
    "\n",
    "for i in range(len(full_nm_output)):\n",
    "    co = []\n",
    "    for j in full_nm_output[i]:\n",
    "        if j.encode('ascii','replace') in full_co_output[i]:\n",
    "            co.append(full_co_output[i].index(j.encode('ascii','replace')))\n",
    "    row_index_co.append(co)\n",
    "    \n",
    "\n",
    "for i in range(len(full_nm_output)):\n",
    "    wb = []\n",
    "    for j in full_nm_output[i]:\n",
    "        if j.encode('ascii','replace') in full_wb_output[i]:\n",
    "            wb.append(full_wb_output[i].index(j.encode('ascii','replace')))\n",
    "    row_index_wb.append(wb)\n",
    "\n",
    "\n",
    "for i in range(len(full_nm_output)):\n",
    "    em = []\n",
    "    for j in full_nm_output[i]:\n",
    "        if j.encode('ascii','replace') in full_em_output[i]:\n",
    "            em.append(full_em_output[i].index(j.encode('ascii','replace')))\n",
    "    row_index_em.append(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_output_nm = []\n",
    "split_output_ad = []\n",
    "split_output_co = []\n",
    "split_output_wb = []\n",
    "split_output_em = []\n",
    "\n",
    "for i in range(len(full_nm_output)):\n",
    "    for j in range(len(full_nm_output[i])):\n",
    "        split_output_nm.append(full_nm_output[i][j].encode('ascii','replace'))\n",
    "\n",
    "for i in range(len(row_index_ad)):\n",
    "    start = 0\n",
    "    for index in row_index_ad[i][1:]:\n",
    "        split_output_ad.append(full_ad_output[i][start+1:index])\n",
    "        start = index\n",
    "    \n",
    "    split_output_ad.append(full_ad_output[start+1:])\n",
    "\n",
    "for i in range(len(row_index_co)):\n",
    "    start = 0\n",
    "    for index in row_index_co[i][1:]:\n",
    "        split_output_co.append(full_co_output[i][start+1:index])\n",
    "        start = index\n",
    "    \n",
    "    split_output_co.append(full_co_output[start+1:])\n",
    "\n",
    "for i in range(len(row_index_wb)):\n",
    "    start = 0\n",
    "    for index in row_index_wb[i][1:]:\n",
    "        split_output_wb.append(full_wb_output[i][start+1:index])\n",
    "        start = index\n",
    "    \n",
    "    split_output_wb.append(full_wb_output[start+1:])\n",
    "\n",
    "for i in range(len(row_index_em)):\n",
    "    start = 0\n",
    "    for index in row_index_em[i][1:]:\n",
    "        split_output_em.append(full_em_output[i][start+1:index])\n",
    "        start = index\n",
    "    \n",
    "    split_output_em.append(full_em_output[start+1:])\n",
    "\n",
    "print(len(split_output_nm),split_output_nm[0:3])\n",
    "print(len(split_output_ad),split_output_ad[0:3])\n",
    "print(len(split_output_co),split_output_co[0:3])\n",
    "print(len(split_output_wb),split_output_wb[0:3])\n",
    "print(len(split_output_em),split_output_em[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_output_nm2 = []\n",
    "split_output_ql = []\n",
    "split_output_ql2 = []\n",
    "\n",
    "for i in split_output_nm: \n",
    "    split_output_nm2.append(i.split(\",\")[0])\n",
    "    split_output_ql.append(i.split(\",\")[1].split(\" \"))\n",
    "\n",
    "for i in split_output_ql:\n",
    "    sq = []\n",
    "    for j in range(len(i)):\n",
    "        if i[j]:          \n",
    "            sq.append(i[j])\n",
    "    split_output_ql2.append(sq)\n",
    "\n",
    "split_output_ad2 = []\n",
    "\n",
    "for i in range(len(split_output_ad)):\n",
    "    ad = []\n",
    "    for j in range(len(split_output_ad[i])):\n",
    "        ad.append(split_output_ad[i][j].encode('ascii','replace'))\n",
    "    split_output_ad2.append(ad)\n",
    "\n",
    "split_output_co2 = []\n",
    "\n",
    "for i in range(len(split_output_co)):\n",
    "    if split_output_co[i]:\n",
    "        for j in range(len(split_output_co[i])):\n",
    "            split_output_co2.append(split_output_co[i][j].encode('ascii','replace'))  \n",
    "    else:\n",
    "        split_output_co2.append(\"\")\n",
    "\n",
    "split_output_em2 = []\n",
    "\n",
    "for i in range(len(split_output_em)):\n",
    "    if split_output_em[i]:\n",
    "        for j in range(len(split_output_em[i])):\n",
    "            split_output_em2.append(split_output_em[i][j].encode('ascii','replace'))\n",
    "    else:\n",
    "        split_output_em2.append(\"\")\n",
    "\n",
    "split_output_wb2 = []\n",
    "\n",
    "for i in range(len(split_output_wb)):\n",
    "    if split_output_wb[i]:\n",
    "        wb = \"\"\n",
    "        for j in range(len(split_output_wb[i])):\n",
    "            wb += split_output_wb[i][j].encode('ascii','replace')\n",
    "        split_output_wb2.append(wb)                \n",
    "    else:\n",
    "        split_output_wb2.append(\"\")\n",
    "\n",
    "firms_matrix_IFA = []\n",
    "\n",
    "for i in range(len(split_output_nm)):\n",
    "    firms_matrix_IFA.append([split_output_nm2[i],split_output_ql2[i],split_output_ad2[i],split_output_co2[i],split_output_wb2[i],split_output_em2[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(firms_matrix_IFA[0])\n",
    "print(len(firms_matrix_IFA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_matrix_IFA = []\n",
    "for i in firms_matrix_IFA:\n",
    "    if not i in final_matrix_IFA:\n",
    "        final_matrix_IFA.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(final_matrix_IFA[0])\n",
    "print(len(final_matrix_IFA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
