{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>IFA Webscraping Firms</h1>\n",
    "\n",
    "This jupyter notebook contains the code used to webscrape ACCA Firms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importing webscraping libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Defining the Webscraping Spider</h2>\n",
    "\n",
    "This code block is defining what our web spider will do - note that it isn't running it, just defining it. See that we are extending the exsiting scrapy.Spider class rather than doing everything from scratch, so we only have minimal coding to do.\n",
    "\n",
    "We tested this code using one page:\n",
    "\n",
    "https://www.ifa.org.uk/find-an-accountant?p=AL&r=0&i=True&pg=0\n",
    "\n",
    "IFA are helpful enough to return all of their search results as part of the map element on the page, so parsing is the only real issue. We can do one search per postcde and it would only take 121 requests to get the details of all firms, but we will have to remove duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_UK_POSTCODES = ['AB','AL','B','BA','BB','BD','BH','BL','BN','BR','BS','BT','CA','CB','CF','CH','CM','CO','CR','CT','CV','CW','DA','DD','DE','DG','DH','DL','DN','DT','DY','E','EC','EH','EN','EX','FK','FY','G','GL','GU','HA','HD','HG','HP','HR','HS','HU','HX','IG','IP','IV','KA','KT','KW','KY','L','LA','LD','LE','LL','LN','LS','LU','M','ME','MK','ML','N','NE','NG','NN','NP','NR','NW','OL','OX','PA','PE','PH','PL','PO','PR','RG','RH','RM','S','SA','SE','SG','SK','SL','SM','SN','SO','SP','SR','SS','ST','SW','SY','TA','TD','TF','TN','TQ','TR','TS','TW','UB','W','WA','WC','WD','WF','WN','WR','WS','WV','YO','ZE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MySpider(scrapy.Spider):\n",
    "    name = \"IFA\"\n",
    "\n",
    "    custom_settings = {\n",
    "        'DOWNLOAD_DELAY': 1,\n",
    "    }\n",
    "    \n",
    "    def start_requests(self):\n",
    "        \n",
    "        urls = []\n",
    "        POSTCODES = ALL_UK_POSTCODES\n",
    "        \n",
    "        for i in POSTCODES:\n",
    "            url = 'https://www.ifa.org.uk/find-an-accountant?p='+str(i)+'&r=0&i=True&pg=0'\n",
    "            urls.append(url)\n",
    "        \n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "            \n",
    "    def parse(self, response):\n",
    "        f_name = response.xpath('//div[@class=\"map-location\"]//div[@class=\"fa-name\"]//text()').extract()\n",
    "        f_address = response.xpath('//div[@class=\"map-location\"]//div[@class=\"fa-name\"]//text() | //div[@class=\"map-location\"]//div[@class=\"fa-address\"]//text()').extract()\n",
    "        f_company = response.xpath('//div[@class=\"map-location\"]//div[@class=\"fa-name\"]//text() | //div[@class=\"map-location\"]//div[@class=\"fa-company\"]//text()').extract()\n",
    "        f_web = response.xpath('//div[@class=\"map-location\"]//div[@class=\"fa-name\"]//text() | //div[@class=\"map-location\"]//div[@class=\"fa-web\"]//text()').extract()\n",
    "        f_email = response.xpath('//div[@class=\"map-location\"]//div[@class=\"fa-name\"]//text() | //div[@class=\"map-location\"]//div[@class=\"fa-email\"]//text()').extract()\n",
    "            \n",
    "        firm_name = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_name]\n",
    "        firm_address = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_address]\n",
    "        firm_company = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_company]\n",
    "        firm_web = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_web]\n",
    "        firm_email = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_email]\n",
    "                        \n",
    "        full_nm_output.append(firm_name)\n",
    "        full_ad_output.append(firm_address)     \n",
    "        full_co_output.append(firm_company)\n",
    "        full_wb_output.append(firm_web)     \n",
    "        full_em_output.append(firm_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Running the Webscraping</h2>\n",
    "\n",
    "Note, you can't re run the code below in a single session for one reason or another, so you need to restart the kernel between runs.\n",
    "\n",
    "This code creates a lightweight container for our webspider and then runs it - to be honest understanding this is probably optional unless it breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-24 21:33:58 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\n",
      "2017-09-24 21:33:58 [scrapy.utils.log] INFO: Overridden settings: {}\n",
      "2017-09-24 21:33:58 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-09-24 21:33:58 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-09-24 21:33:58 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-09-24 21:33:58 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-09-24 21:33:58 [scrapy.core.engine] INFO: Spider opened\n",
      "2017-09-24 21:33:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-09-24 21:33:58 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6042\n",
      "2017-09-24 21:34:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.ifa.org.uk/find-an-accountant?p=AB&r=0&i=True&pg=0> (referer: None)\n",
      "2017-09-24 21:34:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.ifa.org.uk/find-an-accountant?p=AL&r=0&i=True&pg=0> (referer: None)\n",
      "2017-09-24 21:34:04 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2017-09-24 21:34:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 502,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 302134,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 9, 24, 20, 34, 4, 280606),\n",
      " 'log_count/DEBUG': 3,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2017, 9, 24, 20, 33, 58, 813439)}\n",
      "2017-09-24 21:34:04 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "full_nm_output = []\n",
    "full_ad_output = []\n",
    "full_co_output = []\n",
    "full_wb_output = []\n",
    "full_em_output = []\n",
    "\n",
    "process = CrawlerProcess()\n",
    "process.crawl(MySpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now downloaded all the pages that we want to scrape. The first thing to do is to examine what we got back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Mr Michael Walker, FFA FFTA', u'Mr Ronald Simpson, FFA FIPA']\n"
     ]
    }
   ],
   "source": [
    "#print(full_nm_output[0][0:2])\n",
    "#print(full_ad_output[0][0:8])\n",
    "#print(full_co_output[0][0:8])\n",
    "#print(full_wb_output[0][0:8])\n",
    "#print(full_em_output[0][0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_index_ad = []\n",
    "row_index_co = []\n",
    "row_index_wb = []\n",
    "row_index_em = []\n",
    "\n",
    "for i in range(len(full_nm_output)):\n",
    "    ad = []\n",
    "    for j in full_nm_output[i]:\n",
    "        if j.encode('ascii','replace') in full_ad_output[i]:\n",
    "            ad.append(full_ad_output[i].index(j.encode('ascii','replace')))\n",
    "    row_index_ad.append(ad)\n",
    "\n",
    "\n",
    "for i in range(len(full_nm_output)):\n",
    "    co = []\n",
    "    for j in full_nm_output[i]:\n",
    "        if j.encode('ascii','replace') in full_co_output[i]:\n",
    "            co.append(full_co_output[i].index(j.encode('ascii','replace')))\n",
    "    row_index_co.append(co)\n",
    "    \n",
    "\n",
    "for i in range(len(full_nm_output)):\n",
    "    wb = []\n",
    "    for j in full_nm_output[i]:\n",
    "        if j.encode('ascii','replace') in full_wb_output[i]:\n",
    "            wb.append(full_wb_output[i].index(j.encode('ascii','replace')))\n",
    "    row_index_wb.append(wb)\n",
    "\n",
    "\n",
    "for i in range(len(full_nm_output)):\n",
    "    em = []\n",
    "    for j in full_nm_output[i]:\n",
    "        if j.encode('ascii','replace') in full_em_output[i]:\n",
    "            em.append(full_em_output[i].index(j.encode('ascii','replace')))\n",
    "    row_index_em.append(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205, ['Mr Michael Walker, FFA FFTA', 'Mr Ronald Simpson, FFA FIPA', 'Mr Robert Gordon, FFA FFTA'])\n",
      "(205, [[u'75 Bon Accord Street', u'Aberdeen, Aberdeenshire AB11 6ED', u'UNITED KINGDOM'], [u'19 Stonefield Drive', u'Netherblackhall Inverurie', u'Inverurie, Aberdeenshire AB51 4DZ', u'UNITED KINGDOM'], [u'9 Victoria Street', u'Aberdeen, Scotland AB10 1XB', u'UNITED KINGDOM']])\n",
      "(205, [[u'Michael J Walker & Co'], [u'Simron Consulting'], [u'A G Accounting Limited']])\n",
      "(205, [[''], [''], ['']])\n",
      "(205, [[u'mike@michaeljwalker.co.uk'], [u'simronltd@aol.com'], [u'fixedfeeag@aol.com']])\n"
     ]
    }
   ],
   "source": [
    "split_output_nm = []\n",
    "split_output_ad = []\n",
    "split_output_co = []\n",
    "split_output_wb = []\n",
    "split_output_em = []\n",
    "\n",
    "for i in range(len(full_nm_output)):\n",
    "    for j in range(len(full_nm_output[i])):\n",
    "        split_output_nm.append(full_nm_output[i][j].encode('ascii','replace'))\n",
    "\n",
    "for i in range(len(row_index_ad)):\n",
    "    start = 0\n",
    "    for index in row_index_ad[i][1:]:\n",
    "        split_output_ad.append(full_ad_output[i][start+1:index])\n",
    "        start = index\n",
    "    \n",
    "    split_output_ad.append(full_ad_output[start+1:])\n",
    "\n",
    "for i in range(len(row_index_co)):\n",
    "    start = 0\n",
    "    for index in row_index_co[i][1:]:\n",
    "        split_output_co.append(full_co_output[i][start+1:index])\n",
    "        start = index\n",
    "    \n",
    "    split_output_co.append(full_co_output[start+1:])\n",
    "\n",
    "for i in range(len(row_index_wb)):\n",
    "    start = 0\n",
    "    for index in row_index_wb[i][1:]:\n",
    "        split_output_wb.append(full_wb_output[i][start+1:index])\n",
    "        start = index\n",
    "    \n",
    "    split_output_wb.append(full_wb_output[start+1:])\n",
    "\n",
    "for i in range(len(row_index_em)):\n",
    "    start = 0\n",
    "    for index in row_index_em[i][1:]:\n",
    "        split_output_em.append(full_em_output[i][start+1:index])\n",
    "        start = index\n",
    "    \n",
    "    split_output_em.append(full_em_output[start+1:])\n",
    "\n",
    "print(len(split_output_nm),split_output_nm[0:3])\n",
    "print(len(split_output_ad),split_output_ad[0:3])\n",
    "print(len(split_output_co),split_output_co[0:3])\n",
    "print(len(split_output_wb),split_output_wb[0:3])\n",
    "print(len(split_output_em),split_output_em[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_output_nm2 = []\n",
    "split_output_ql = []\n",
    "split_output_ql2 = []\n",
    "\n",
    "for i in split_output_nm: \n",
    "    split_output_nm2.append(i.split(\",\")[0])\n",
    "    split_output_ql.append(i.split(\",\")[1].split(\" \"))\n",
    "\n",
    "for i in split_output_ql:\n",
    "    sq = []\n",
    "    for j in range(len(i)):\n",
    "        if i[j]:          \n",
    "            sq.append(i[j])\n",
    "    split_output_ql2.append(sq)\n",
    "\n",
    "split_output_ad2 = []\n",
    "\n",
    "for i in range(len(split_output_ad)):\n",
    "    ad = []\n",
    "    for j in range(len(split_output_ad[i])):\n",
    "        ad.append(split_output_ad[i][j].encode('ascii','replace'))\n",
    "    split_output_ad2.append(ad)\n",
    "\n",
    "split_output_co2 = []\n",
    "\n",
    "for i in range(len(split_output_co)):\n",
    "    if split_output_co[i]:\n",
    "        for j in range(len(split_output_co[i])):\n",
    "            split_output_co2.append(split_output_co[i][j].encode('ascii','replace'))  \n",
    "    else:\n",
    "        split_output_co2.append(\"\")\n",
    "\n",
    "split_output_em2 = []\n",
    "\n",
    "for i in range(len(split_output_em)):\n",
    "    if split_output_em[i]:\n",
    "        for j in range(len(split_output_em[i])):\n",
    "            split_output_em2.append(split_output_em[i][j].encode('ascii','replace'))\n",
    "    else:\n",
    "        split_output_em2.append(\"\")\n",
    "\n",
    "split_output_wb2 = []\n",
    "\n",
    "for i in range(len(split_output_wb)):\n",
    "    if split_output_wb[i]:\n",
    "        wb = \"\"\n",
    "        for j in range(len(split_output_wb[i])):\n",
    "            wb += split_output_wb[i][j].encode('ascii','replace')\n",
    "        split_output_wb2.append(wb)                \n",
    "    else:\n",
    "        split_output_wb2.append(\"\")\n",
    "\n",
    "firms_matrix_IFA = []\n",
    "\n",
    "for i in range(len(split_output_nm)):\n",
    "    firms_matrix_IFA.append([split_output_nm2[i],split_output_ql2[i],split_output_ad2[i],split_output_co2[i],split_output_wb2[i],split_output_em2[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr Michael Walker', ['FFA', 'FFTA'], ['75 Bon Accord Street', 'Aberdeen, Aberdeenshire AB11 6ED', 'UNITED KINGDOM'], 'Michael J Walker & Co', '', 'mike@michaeljwalker.co.uk']\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "print(firms_matrix_IFA[0])\n",
    "print(len(firms_matrix_IFA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_matrix_IFA = []\n",
    "for i in firms_matrix_IFA:\n",
    "    if not i in final_matrix_IFA:\n",
    "        final_matrix_IFA.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr Michael Walker', ['FFA', 'FFTA'], ['75 Bon Accord Street', 'Aberdeen, Aberdeenshire AB11 6ED', 'UNITED KINGDOM'], 'Michael J Walker & Co', '', 'mike@michaeljwalker.co.uk']\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "print(final_matrix_IFA[0])\n",
    "print(len(final_matrix_IFA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
