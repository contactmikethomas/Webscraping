{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>AIA Webscraping Firms</h1>\n",
    "\n",
    "This jupyter notebook contains the code used to webscrape AIA Firms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importing webscraping libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Defining the Webscraping Spider</h2>\n",
    "\n",
    "This code block is defining what our web spider will do - note that it isn't running it, just defining it. See that we are extending the exsiting scrapy.Spider class rather than doing everything from scratch, so we only have minimal coding to do.\n",
    "\n",
    "We tested this code using one page:\n",
    "\n",
    "http://www.aiaworldwide.com/find-an-international-accountant\n",
    "\n",
    "AIA are helpful enough to just list all of their members on one page, but the details are relatively limited. Luckily each member has a link to their full profile, so we can scrape those links as stage one and then use those links to get the remaining details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Ran this code once to get url list, which is saved\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"AIA\"\n",
    "    \n",
    "    def start_requests(self):\n",
    "        \n",
    "        urls = []\n",
    "        \n",
    "        url = 'http://www.aiaworldwide.com/find-an-international-accountant'\n",
    "        urls.append(url)\n",
    "        \n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "            \n",
    "    def parse(self, response):\n",
    "        f_url = response.xpath('//tbody//tr//a//@href').extract()\n",
    "            \n",
    "        firm_url = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_url]\n",
    "                        \n",
    "        full_url_output.append(firm_url)\n",
    "        \n",
    "full_url_output = []\n",
    "\n",
    "process = CrawlerProcess()\n",
    "process.crawl(MySpider)\n",
    "process.start()\n",
    "\n",
    "final_url_output = []\n",
    "\n",
    "for i in full_url_output[0]:\n",
    "    final_url_output.append(i.encode('ascii','replace'))\n",
    "    \n",
    "f = open('/home/de-admin/Documents/Webscraping/AIA_links.txt', 'w')\n",
    "\n",
    "for url in final_url_output:\n",
    "    print>>f, url\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Defining the Webscraping Spider</h2>\n",
    "\n",
    "We now have the list of urls and can proceed to the final webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/de-admin/Documents/Webscraping/AIA_links.txt') as f:\n",
    "    url_saved = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class MySpider2(scrapy.Spider):\n",
    "    name = \"AIA_FIRMS\"\n",
    "    \n",
    "    custom_settings = {\n",
    "        'DOWNLOAD_DELAY': 1,\n",
    "    }\n",
    "    \n",
    "    def start_requests(self):\n",
    "        \n",
    "        urls = []\n",
    "        \n",
    "        for i in url_saved[10:13]:\n",
    "            url = 'http://www.aiaworldwide.com'+i\n",
    "            urls.append(url)\n",
    "                \n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "            \n",
    "    def parse(self, response):\n",
    "        f_titl = response.xpath('//div[@class=\"field field-name-field-person-title field-type-taxonomy-term-reference field-label-inline clearfix\"]//div[@class=\"field-item even\"]//text()').extract()\n",
    "        f_init = response.xpath('//div[@class=\"field field-name-field-person-initials field-type-text field-label-inline clearfix\"]//div[@class=\"field-item even\"]//text()').extract()\n",
    "        f_name = response.xpath('//h1//text()').extract()        \n",
    "        f_qual = response.xpath('//div[@class=\"field field-name-field-designatory-letters field-type-text field-label-above\"]//div[@class=\"field-item even\"]//text()').extract()\n",
    "        f_comp = response.xpath('//div[@class=\"field field-name-field-company-name field-type-text field-label-inline clearfix\"]//div[@class=\"field-item even\"]//text()').extract()\n",
    "        f_town = response.xpath('//div[@class=\"field field-name-field-city field-type-text field-label-inline clearfix\"]//div[@class=\"field-item even\"]//text()').extract()\n",
    "        f_ctry = response.xpath('//div[@class=\"field field-name-field-country field-type-country field-label-inline clearfix\"]//div[@class=\"field-item even\"]//text()').extract()\n",
    "        f_tphn = response.xpath('//div[@class=\"field field-name-field-telephone field-type-text field-label-inline clearfix\"]//div[@class=\"field-item even\"]//text()').extract()\n",
    "        f_faxn = response.xpath('//div[@class=\"field field-name-field-fax field-type-text field-label-inline clearfix\"]//div[@class=\"field-item even\"]//text()').extract()\n",
    "        f_webs = response.xpath('//div[@class=\" field field-name-field-website field-type-link-field field-label-inline clearfix\"]//div[@class=\"field-item even\"]//text()').extract()\n",
    "        f_stat = response.xpath('//div[@class=\"field field-name-field-image field-type-image field-label-hidden\"]//img//@alt').extract()     \n",
    "        \n",
    "        firm_title = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_titl]\n",
    "        firm_initials = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_init]\n",
    "        firm_name = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_name]\n",
    "        firm_qualifications = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_qual]\n",
    "        firm_company = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_comp]\n",
    "        firm_town = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_town]\n",
    "        firm_country = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_ctry]\n",
    "        firm_telephone = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_tphn]\n",
    "        firm_fax = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_faxn]\n",
    "        firm_website = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_webs]\n",
    "        firm_status = [' '.join(a.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').split()) for a in f_stat]\n",
    "        \n",
    "        full_firm_output.append([firm_title,firm_initials,firm_name,firm_qualifications,firm_company,firm_town,firm_country,firm_telephone,firm_fax,firm_website,firm_status])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Running the Webscraping</h2>\n",
    "\n",
    "Note, you can't re run the code below in a single session for one reason or another, so you need to restart the kernel between runs.\n",
    "\n",
    "This code creates a lightweight container for our webspider and then runs it - to be honest understanding this is probably optional unless it breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add fax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-25 12:23:55 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\n",
      "2017-09-25 12:23:55 [scrapy.utils.log] INFO: Overridden settings: {}\n",
      "2017-09-25 12:23:55 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-09-25 12:23:55 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-09-25 12:23:55 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-09-25 12:23:55 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-09-25 12:23:55 [scrapy.core.engine] INFO: Spider opened\n",
      "2017-09-25 12:23:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-09-25 12:23:55 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6042\n",
      "2017-09-25 12:23:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.aiaworldwide.com/njini> (referer: None)\n",
      "2017-09-25 12:23:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.aiaworldwide.com/bowman> (referer: None)\n",
      "2017-09-25 12:23:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.aiaworldwide.com/gench> (referer: None)\n",
      "2017-09-25 12:23:59 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2017-09-25 12:23:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 670,\n",
      " 'downloader/request_count': 3,\n",
      " 'downloader/request_method_count/GET': 3,\n",
      " 'downloader/response_bytes': 114276,\n",
      " 'downloader/response_count': 3,\n",
      " 'downloader/response_status_count/200': 3,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 9, 25, 11, 23, 59, 70208),\n",
      " 'log_count/DEBUG': 4,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 3,\n",
      " 'scheduler/dequeued': 3,\n",
      " 'scheduler/dequeued/memory': 3,\n",
      " 'scheduler/enqueued': 3,\n",
      " 'scheduler/enqueued/memory': 3,\n",
      " 'start_time': datetime.datetime(2017, 9, 25, 11, 23, 55, 369130)}\n",
      "2017-09-25 12:23:59 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "full_firm_output = []\n",
    "\n",
    "process = CrawlerProcess()\n",
    "process.crawl(MySpider2)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now downloaded all the pages that we want to scrape. The first thing to do is to examine what we got back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[u'Mr'], [u'GY'], [u'Njini'], [u'FAIA ACIS FCEA FIPFM MBA'], [u'Accountancy Services & Audit (ASA Group)'], [u'Bamenda'], [u'Cameroon'], [u'00237 7743 9468'], [u'Member in Practice']], [[u'Mr'], [u'SD'], [u'Bowman'], [u'FAIA FFA FMAAT'], [u'Stephen Bowman Associates'], [u'Bangor'], [u'United Kingdom'], [u'02891 459 455'], [u'Member in Practice']], [[u'Mr'], [u'GG'], [u'Gench'], [u'MIL FAIA'], [u'Gench & Company'], [u'Barking'], [u'United Kingdom'], [u'020 8220 6614'], []]]\n"
     ]
    }
   ],
   "source": [
    "print(full_firm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "row_index_ad = []\n",
    "row_index_co = []\n",
    "row_index_wb = []\n",
    "row_index_em = []\n",
    "\n",
    "for i in range(len(full_nm_output)):\n",
    "    ad = []\n",
    "    for j in full_nm_output[i]:\n",
    "        if j.encode('ascii','replace') in full_ad_output[i]:\n",
    "            ad.append(full_ad_output[i].index(j.encode('ascii','replace')))\n",
    "    row_index_ad.append(ad)\n",
    "\n",
    "\n",
    "for i in range(len(full_nm_output)):\n",
    "    co = []\n",
    "    for j in full_nm_output[i]:\n",
    "        if j.encode('ascii','replace') in full_co_output[i]:\n",
    "            co.append(full_co_output[i].index(j.encode('ascii','replace')))\n",
    "    row_index_co.append(co)\n",
    "    \n",
    "\n",
    "for i in range(len(full_nm_output)):\n",
    "    wb = []\n",
    "    for j in full_nm_output[i]:\n",
    "        if j.encode('ascii','replace') in full_wb_output[i]:\n",
    "            wb.append(full_wb_output[i].index(j.encode('ascii','replace')))\n",
    "    row_index_wb.append(wb)\n",
    "\n",
    "\n",
    "for i in range(len(full_nm_output)):\n",
    "    em = []\n",
    "    for j in full_nm_output[i]:\n",
    "        if j.encode('ascii','replace') in full_em_output[i]:\n",
    "            em.append(full_em_output[i].index(j.encode('ascii','replace')))\n",
    "    row_index_em.append(em)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "split_output_nm = []\n",
    "split_output_ad = []\n",
    "split_output_co = []\n",
    "split_output_wb = []\n",
    "split_output_em = []\n",
    "\n",
    "for i in range(len(full_nm_output)):\n",
    "    for j in range(len(full_nm_output[i])):\n",
    "        split_output_nm.append(full_nm_output[i][j].encode('ascii','replace'))\n",
    "\n",
    "for i in range(len(row_index_ad)):\n",
    "    start = 0\n",
    "    for index in row_index_ad[i][1:]:\n",
    "        split_output_ad.append(full_ad_output[i][start+1:index])\n",
    "        start = index\n",
    "    \n",
    "    split_output_ad.append(full_ad_output[start+1:])\n",
    "\n",
    "for i in range(len(row_index_co)):\n",
    "    start = 0\n",
    "    for index in row_index_co[i][1:]:\n",
    "        split_output_co.append(full_co_output[i][start+1:index])\n",
    "        start = index\n",
    "    \n",
    "    split_output_co.append(full_co_output[start+1:])\n",
    "\n",
    "for i in range(len(row_index_wb)):\n",
    "    start = 0\n",
    "    for index in row_index_wb[i][1:]:\n",
    "        split_output_wb.append(full_wb_output[i][start+1:index])\n",
    "        start = index\n",
    "    \n",
    "    split_output_wb.append(full_wb_output[start+1:])\n",
    "\n",
    "for i in range(len(row_index_em)):\n",
    "    start = 0\n",
    "    for index in row_index_em[i][1:]:\n",
    "        split_output_em.append(full_em_output[i][start+1:index])\n",
    "        start = index\n",
    "    \n",
    "    split_output_em.append(full_em_output[start+1:])\n",
    "\n",
    "print(len(split_output_nm),split_output_nm[0:3])\n",
    "print(len(split_output_ad),split_output_ad[0:3])\n",
    "print(len(split_output_co),split_output_co[0:3])\n",
    "print(len(split_output_wb),split_output_wb[0:3])\n",
    "print(len(split_output_em),split_output_em[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "split_output_nm2 = []\n",
    "split_output_ql = []\n",
    "split_output_ql2 = []\n",
    "\n",
    "for i in split_output_nm: \n",
    "    split_output_nm2.append(i.split(\",\")[0])\n",
    "    split_output_ql.append(i.split(\",\")[1].split(\" \"))\n",
    "\n",
    "for i in split_output_ql:\n",
    "    sq = []\n",
    "    for j in range(len(i)):\n",
    "        if i[j]:          \n",
    "            sq.append(i[j])\n",
    "    split_output_ql2.append(sq)\n",
    "\n",
    "split_output_ad2 = []\n",
    "\n",
    "for i in range(len(split_output_ad)):\n",
    "    ad = []\n",
    "    for j in range(len(split_output_ad[i])):\n",
    "        ad.append(split_output_ad[i][j].encode('ascii','replace'))\n",
    "    split_output_ad2.append(ad)\n",
    "\n",
    "split_output_co2 = []\n",
    "\n",
    "for i in range(len(split_output_co)):\n",
    "    if split_output_co[i]:\n",
    "        for j in range(len(split_output_co[i])):\n",
    "            split_output_co2.append(split_output_co[i][j].encode('ascii','replace'))  \n",
    "    else:\n",
    "        split_output_co2.append(\"\")\n",
    "\n",
    "split_output_em2 = []\n",
    "\n",
    "for i in range(len(split_output_em)):\n",
    "    if split_output_em[i]:\n",
    "        for j in range(len(split_output_em[i])):\n",
    "            split_output_em2.append(split_output_em[i][j].encode('ascii','replace'))\n",
    "    else:\n",
    "        split_output_em2.append(\"\")\n",
    "\n",
    "split_output_wb2 = []\n",
    "\n",
    "for i in range(len(split_output_wb)):\n",
    "    if split_output_wb[i]:\n",
    "        wb = \"\"\n",
    "        for j in range(len(split_output_wb[i])):\n",
    "            wb += split_output_wb[i][j].encode('ascii','replace')\n",
    "        split_output_wb2.append(wb)                \n",
    "    else:\n",
    "        split_output_wb2.append(\"\")\n",
    "\n",
    "firms_matrix_IFA = []\n",
    "\n",
    "for i in range(len(split_output_nm)):\n",
    "    firms_matrix_IFA.append([split_output_nm2[i],split_output_ql2[i],split_output_ad2[i],split_output_co2[i],split_output_wb2[i],split_output_em2[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(firms_matrix_IFA[0])\n",
    "print(len(firms_matrix_IFA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "final_matrix_IFA = []\n",
    "for i in firms_matrix_IFA:\n",
    "    if not i in final_matrix_IFA:\n",
    "        final_matrix_IFA.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(final_matrix_IFA[0])\n",
    "print(len(final_matrix_IFA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
