{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ICAEW Webscraping Test</h1>\n",
    "\n",
    "This jupyter notebook contains our experiments to webscrape all of the Professional Body data that we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importing webscraping libraries</h2>\n",
    "\n",
    "The first time that you do webscraping you may have to download some tools, I chose to experiment with scrapy:\n",
    "\n",
    "https://scrapy.org/\n",
    "\n",
    "I'm running this on a machine with Anaconda and Jupyter, so all I need to do to install scrapy is to type:\n",
    "\n",
    "conda install scrapy\n",
    "\n",
    "And let anadona do the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the installation worked the the import statement above runs with no output. We're going to try and follow the scrapy tutorial on these pages:\n",
    "\n",
    "https://docs.scrapy.org/en/latest/intro/tutorial.html\n",
    "\n",
    "https://docs.scrapy.org/en/latest/topics/practices.html#run-scrapy-from-a-script\n",
    "\n",
    "So, let's get this going."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Defining the Webscraping Spider</h2>\n",
    "\n",
    "This code block is defining what our web spider will do.\n",
    "\n",
    "We tested this code using this page: http://www.accaglobal.com/uk/en/member/find-an-accountant/find-firm/results/details.html?isocountry=GB&location=London&country=UK&advisorid=2841947\n",
    "\n",
    "https://find.icaew.com/listings/view/listing_id/35931 because it has several complex features that are not available on every page.\n",
    "\n",
    "<h3>start_requests</h3>\n",
    "\n",
    "The first function has two purposes:\n",
    "\n",
    "The first is to construct the series of urls that we are going to scrape. Note that because of the design of the website we just have to increase the id number by one to try a new page. Not every single ID number is currently in use, but the code will just skip a missing page by itself.\n",
    "\n",
    "The second is to iterate over the list of urls using the Request function in scrapy which takes at least a url and a function as arguements. The function is just the set of instructions we've built to get the information we want from the page - we'll discuss this in a moment.\n",
    "\n",
    "<p style=\"color:red\">To do, impliment the better extraction method<br>\n",
    "To do, mute the log</p>\n",
    "\n",
    "<h3>parse</h3>\n",
    "\n",
    "We second function has one real pupose, which is to read the web page we access, scrape the useful information and package it up as a neat data structure - we don't really need to neaten it up here, but the processing is trivial and it will reduce the size of the output. The three steps work like this:\n",
    "\n",
    "<h4>1. scrape the important data</h4>\n",
    "\n",
    "\"page = response.url.split(\"/\")[-1]\" extracts everything after the final slash in the URL, which in this case is the membership number!\n",
    "\n",
    "\"response\" is a class which allows to manipulate the data in the webpage we call. In this case were going to use a \"selecter\" method which lets us pick individual elements of the webpage, and then the extract method to pick out the selected element. We have used the xpath selecter because the page is html, but you can use other ones (e.g. css).\n",
    "\n",
    "It's too mucht to explain xpath here, but I will explain one as a guide. Obviously the exact coding here depends completely on the web page being scraped and took a little while to develop. We're lucky that the webpages we're scraping are very regular in format, so we can get what we need without too much bother.\n",
    "\n",
    "'//div[@class=\"row-fluid listing-details\"]//div[@class=\"span3 title\"]//text() | //ul[@class=\"prof\"]//text()'\n",
    "\n",
    "The first thing to note is that this has a pipe character in it. We're actually extracting two types of data - the first is the heading of the section (which says \"specialisms\"), and the second is a list of the specialisms. This is convenient because not all data is available for every firm, so it makes it simpler to identify what we're meant to be looking at.\n",
    "\n",
    "So, from the left: \n",
    "\n",
    "\"//div[@class=\"row-fluid listing-details\"]\" means \"find all div tags with the class \"row-fluid lisitng details\"\". It's the \"//\" which means \"any\", you could also use \"/\" for \"the first\". The \"=\" means find an identical match, but you can use other comparitors.\n",
    "\n",
    "\"//div[@class=\"span3 title\"]\" means (having narrowed the search above) to \"find all div tags with the class \"span3 title\"\". In our website that is only one place in the document.\n",
    "\n",
    "\"//text()\" means select all text elements inside the tags we have selected.\n",
    "\n",
    "In this case the output is a messy looking string that contains the text we want, some new line characters, and some white space - we'll deal with that in the next block.\n",
    "\n",
    "<h4>2. clean the data</h4>\n",
    "\n",
    "The purpose of the next section of code is to remove the newline characters and whitespace:\n",
    "\n",
    "firm_name = [' '.join(a.replace('\\n', ' ').split()) for a in f_name]\n",
    "\n",
    "For each element in the f_name list, we do two things:\n",
    "\n",
    "Firstly we replace all newline characers with spaces using the .replace method.\n",
    "\n",
    "Secondly, we remove the whitespace - this is done using a combination of the join and split functions. The split function breaks each sentence into individual words, and the join function joins them back together, in this case with a space in between. \n",
    "\n",
    "<h4>3. package the data up.</h4>\n",
    "\n",
    "This step just puts all the data into a list of lists - in fact there is more processing to do but we can do that offline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MySpider(scrapy.Spider):\n",
    "    name = \"ICAEW\"\n",
    "\n",
    "    custom_settings = {\n",
    "        'DOWNLOAD_DELAY': 2,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def start_requests(self):\n",
    "        \n",
    "        urls = []\n",
    "                \n",
    "        for i in range(70,81):\n",
    "            url = 'https://find.icaew.com/listings/view/listing_id/'+str(i)\n",
    "            urls.append(url)\n",
    "        \n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "            \n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-1]      \n",
    "        f_name = response.xpath('//div[@class=\"listing-name\"]/h1/text()').extract()\n",
    "        f_contact_details = response.xpath('//div[@class=\"row-fluid listing-details listing\"]//div[@class=\"span3 title\"]//text() | //div[@class=\"row-fluid listing-details listing\"]//div[@class=\"span9\"]//text()').extract()\n",
    "        f_specialisations = response.xpath('//div[@class=\"row-fluid listing-details\"]//div[@class=\"span3 title\"]//text() | //ul[@class=\"prof\"]//text()').extract()\n",
    "        f_profile = response.xpath('//h2[@class=\"pgttl prof promid\"]//text() | //div[@class=\"span12 profile\"]//text()').extract()\n",
    "        f_qualifications = response.xpath('//div[@class=\"striped listing-details\"]//div[@class=\"span3 title\"]//text() | //div[@class=\"striped listing-details\"]//div[@class=\"span9\"]//text()').extract()\n",
    "        f_partners_and_staff = response.xpath('//h3[@class=\"pgttl prof promid\"]//text() | //h4[@class=\"pgttl prof promid\"]//text() | //div[@class=\"striped listing-details\"]//div[@class=\"span12 title\"]//text()').extract()\n",
    "        \n",
    "        firm_name = [' '.join(a.replace('\\n', ' ').split()) for a in f_name]\n",
    "        firm_contact_details = [' '.join(a.replace('\\n', ' ').split()) for a in f_contact_details]\n",
    "        firm_specialisations = [' '.join(a.replace('\\n', ' ').split()) for a in f_specialisations]\n",
    "        firm_profile = [' '.join(a.replace('\\n', ' ').split()) for a in f_profile]\n",
    "        firm_qualifications = [' '.join(a.replace('\\n', ' ').split()) for a in f_qualifications]\n",
    "        firm_partners_and_staff = [' '.join(a.replace('\\n', ' ').split()) for a in f_partners_and_staff]\n",
    "        \n",
    "        firm = [page,firm_name,firm_contact_details,firm_specialisations,firm_profile,firm_qualifications,firm_partners_and_staff]\n",
    "        firms.append(firm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Running the Webscraping</h2>\n",
    "\n",
    "Note, you can't re run the code below in a single session for one reason or another, so you need to restart the kernel between runs.\n",
    "\n",
    "This code creates a lightweight container for our webspider and then runs it - to be honest understanding this is probably optional unless it breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-22 11:22:20 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\n",
      "2017-09-22 11:22:20 [scrapy.utils.log] INFO: Overridden settings: {}\n",
      "2017-09-22 11:22:20 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-09-22 11:22:20 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-09-22 11:22:20 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-09-22 11:22:20 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-09-22 11:22:20 [scrapy.core.engine] INFO: Spider opened\n",
      "2017-09-22 11:22:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-09-22 11:22:20 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6042\n",
      "2017-09-22 11:22:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://find.icaew.com/listings/view/listing_id/70> (referer: None)\n",
      "2017-09-22 11:22:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://find.icaew.com/listings/view/listing_id/71> (referer: None)\n",
      "2017-09-22 11:22:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://find.icaew.com/listings/view/listing_id/72> (referer: None)\n",
      "2017-09-22 11:22:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://find.icaew.com/listings/view/listing_id/73> (referer: None)\n",
      "2017-09-22 11:22:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://find.icaew.com/listings/view/listing_id/73>: HTTP status code is not handled or not allowed\n",
      "2017-09-22 11:22:31 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://find.icaew.com/listings/view/listing_id/74> (referer: None)\n",
      "2017-09-22 11:22:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://find.icaew.com/listings/view/listing_id/74>: HTTP status code is not handled or not allowed\n",
      "2017-09-22 11:22:33 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://find.icaew.com/listings/view/listing_id/75> (referer: None)\n",
      "2017-09-22 11:22:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://find.icaew.com/listings/view/listing_id/75>: HTTP status code is not handled or not allowed\n",
      "2017-09-22 11:22:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://find.icaew.com/listings/view/listing_id/76> (referer: None)\n",
      "2017-09-22 11:22:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://find.icaew.com/listings/view/listing_id/77> (referer: None)\n",
      "2017-09-22 11:22:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://find.icaew.com/listings/view/listing_id/78> (referer: None)\n",
      "2017-09-22 11:22:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://find.icaew.com/listings/view/listing_id/79> (referer: None)\n",
      "2017-09-22 11:22:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://find.icaew.com/listings/view/listing_id/80> (referer: None)\n",
      "2017-09-22 11:22:46 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2017-09-22 11:22:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2629,\n",
      " 'downloader/request_count': 11,\n",
      " 'downloader/request_method_count/GET': 11,\n",
      " 'downloader/response_bytes': 289628,\n",
      " 'downloader/response_count': 11,\n",
      " 'downloader/response_status_count/200': 8,\n",
      " 'downloader/response_status_count/404': 3,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 9, 22, 10, 22, 46, 682301),\n",
      " 'log_count/DEBUG': 12,\n",
      " 'log_count/INFO': 10,\n",
      " 'response_received_count': 11,\n",
      " 'scheduler/dequeued': 11,\n",
      " 'scheduler/dequeued/memory': 11,\n",
      " 'scheduler/enqueued': 11,\n",
      " 'scheduler/enqueued/memory': 11,\n",
      " 'start_time': datetime.datetime(2017, 9, 22, 10, 22, 20, 589401)}\n",
      "2017-09-22 11:22:46 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "firms = []\n",
    "process = CrawlerProcess()\n",
    "process.crawl(MySpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now downloaded all the pages that we want to scrape. The first thing to do is to examine what we got back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to parse the data\n",
    "\n",
    "[4] - removing blanks, and concatenating\n",
    "[3] - only one example in the top 100 - need to check, removing blanks\n",
    "[2] - a set of pairs, but not all mandatory so we need to work out what exists and then populate it or not.\n",
    "[1] - name, convert from unicode\n",
    "[0] - page ID, as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "des_data = []\n",
    "\n",
    "for i in range(len(firms)):   \n",
    "    if firms[i][2]:\n",
    "        dd = []\n",
    "        for j in range(0,len(firms[i][2])):\n",
    "            if firms[i][2][j]: \n",
    "                dd.append(firms[i][2][j].encode('ascii','replace'))\n",
    "        des_data.append(dd)    \n",
    "    else:\n",
    "        des_data.append(\"\")   \n",
    "\n",
    "#print(des_data)\n",
    "\n",
    "des_type = []\n",
    "\n",
    "for i in range (len(des_data)):\n",
    "    for j in range (0,len(des_data[i]),2):\n",
    "        des_type.append(des_data[i][j].encode('ascii','replace'))    \n",
    "\n",
    "des_set = list(set(des_type))\n",
    "\n",
    "#print(des_set)\n",
    "\n",
    "designatory_data = []\n",
    "\n",
    "for i in range (len(des_data)):\n",
    "    f_des = []\n",
    "    for t in range(len(des_set)):\n",
    "        if des_set[t] in des_data[i]:\n",
    "            f_des.append(des_data[i][des_data[i].index(str(des_set[t])) + 1].encode('ascii','replace'))\n",
    "        else:\n",
    "            f_des.append(\"\")\n",
    "    \n",
    "    designatory_data.append(f_des)\n",
    "\n",
    "#print(designatory_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to extract specialisations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "specialisations = []\n",
    "\n",
    "for i in range(len(firms)):   \n",
    "    if firms[i][3]:\n",
    "        spec = []\n",
    "        for j in range(1,len(firms[i][3])):\n",
    "            if firms[i][3][j]: \n",
    "                spec.append(firms[i][3][j].encode('ascii','replace'))\n",
    "        specialisations.append(spec)    \n",
    "    else:\n",
    "        specialisations.append(\"\")  \n",
    "\n",
    "#print(specialisations) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to extract company profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "company_profile = []\n",
    "\n",
    "for i in range(len(firms)):\n",
    "    if firms[i][4] > 2:\n",
    "        prof = []\n",
    "        for j in range(1,len(firms[i][4])):\n",
    "            if firms[i][4][j]: \n",
    "                prof.append(firms[i][4][j].encode('ascii','replace'))\n",
    "        company_profile.append(' '.join(prof))  \n",
    "    else:\n",
    "        company_profile.append(\"\")  \n",
    "\n",
    "#print(company_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to make the qualifications data into a regular format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qualifications = []\n",
    "\n",
    "qual_type = []\n",
    "\n",
    "for i in range (len(firms)):\n",
    "    for j in range (0,len(firms[i][5]),2):\n",
    "        qual_type.append(firms[i][5][j].encode('ascii','replace'))    \n",
    "\n",
    "qual_set = list(set(qual_type))\n",
    "    \n",
    "#print(qual_set)\n",
    "\n",
    "for i in range (len(firms)):\n",
    "    f_qual = []\n",
    "    for t in range(len(qual_set)):\n",
    "        if qual_set[t] in firms[i][5]:\n",
    "            f_qual.append(firms[i][5][firms[i][5].index(str(qual_set[t])) + 1].encode('ascii','replace'))\n",
    "        else:\n",
    "            f_qual.append(\"\")\n",
    "    qualifications.append(f_qual)\n",
    "    \n",
    "#print(qualifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to extract details of the chartered accountants and other staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "staff_types = ['ICAEW Chartered Accountants','Partners & additional staff']\n",
    "\n",
    "type_index = []\n",
    "\n",
    "for i in range(len(firms)):\n",
    "    type_ind = []\n",
    "    \n",
    "    for j in staff_types:\n",
    "        if [firms[i][6].index(str(j)) for entry in firms[i][6] if j in entry]:\n",
    "            type_ind.append([firms[i][6].index(str(j)) for entry in firms[i][6] if j in entry][0])\n",
    "        else:\n",
    "            type_ind.append(None)\n",
    "    \n",
    "    type_index.append(type_ind)\n",
    "\n",
    "#print(type_index)\n",
    "\n",
    "staff_ICAEW = []\n",
    "staff_other = []\n",
    "\n",
    "for i in range(len(firms)):\n",
    "    \n",
    "    if not type_index[i][0] is None:\n",
    "        staff_I = []\n",
    "        staff_o = []\n",
    "        if not type_index[i][1] is None:\n",
    "            for line in firms[i][6][type_index[i][0]+1:type_index[i][1]]:\n",
    "                staff_I.append(line)\n",
    "            for line in firms[i][6][type_index[i][1]+1:len(firms[i][6])]:\n",
    "                staff_o.append(line)        \n",
    "        else:\n",
    "            for line in firms[i][6][type_index[i][0]+1:len(firms[i][6])]:\n",
    "                staff_I.append(line)\n",
    "            \n",
    "        staff_ICAEW.append(staff_I)\n",
    "        staff_other.append(staff_o)\n",
    "\n",
    "    elif not type_index[i][1] is None:\n",
    "        staff_I = []\n",
    "        staff_o = []\n",
    "\n",
    "        for line in firms[i][6][type_index[i][1]+1:len(firms[i][6])]:\n",
    "            staff_o.append(line)        \n",
    "            \n",
    "        staff_ICAEW.append(staff_I)\n",
    "        staff_other.append(staff_o)    \n",
    "\n",
    "    else:\n",
    "        staff_ICAEW.append([])\n",
    "        staff_other.append([])  \n",
    "\n",
    "#print(staff_ICAEW)\n",
    "#print(staff_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now constructed all the variables we need, so  the final step is to put them into a matrix, sort them for convenience and inspect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[70, 'Plushcourt Estates Limited', '2 The Estate Yard, Ixworth, Bury St. Edmunds', 'IP31 2HE', '', '', '', ['', ''], [], []], [71, 'Davidsons', '23 Comfrey Close, Farnborough', 'GU14 9XX', '01252 514292', '', '', ['', ''], [], []], [72, 'Stephen Mayled & Associates Ltd', 'Cottage Farm, Michaelston-le-Pit, Dinas Powys', 'CF64 4HE', '02920 515777', '', '', ['', ''], [], []], [76, 'Barrowby Accountants Limited', 'Kobia, Low Road, Barrowby, Grantham', 'NG32 1DJ', '01476 569832', '', '', ['', ''], [], []], [77, 'Potter and Pollard', 'Suite 7, Wessex House, St. Leonards Road, Bournemouth', 'BH8 8QS', '01202 526677', '', '', ['Registered by the ICAEW to carry out audit work', 'Regulated by ICAEW for a range of investment business activities'], [u'Mrs Janet Gee', u'Mrs Janet Gee', u'Mrs Janet Gee'], []], [78, 'PricewaterhouseCoopers LLP', '1 Embankment Place, London', 'WC2N 6RH', '0207 583 5000', '', '', ['Registered by the ICAEW to carry out audit work', ''], [u'Mr Philip Bloomfield', u'Mr Timothy McCann', u'Mr Anthony Meeke', u\"Mr Brendan O'Driscoll\", u\"Mr Neil O'Keeffe\", u'Mr James Quin', u'Mr William Richardson', u'Mr Lachlan Roos', u'Miss Anne Stephenson', u'Mr Daniel Stott', u'Mr Roger Thomas', u'Mr John Wayman', u'Mr Ben Wilkins'], [u'Amit Abhyankar', u'Amit Abhyankar', u'Arif Ahmad', u'Philippa Booth', u'Kevin Burrowes', u'Kevin Burrowes', u'Nigel Comello', u'Stephen Denison', u'Ewan Fryatt', u'Hemione Hudson', u'Richard Hunter', u'William Jackson-Moore', u'William Jackson-Moore', u'Rahul Patel', u'Rohit Patiar', u'David Reeman', u'David Reeman', u'Richard Siddall', u'Charles Skelton', u'Charles Skelton', u'Sarah Taylor', u'Sarah Taylor', u'Victoria Tillbrook', u'Victoria Tillbrook', u'Max Torpy', u'Max Torpy', u'James Tubby', u'James Tubby', u'Simon Viner', u'Simon Viner']], [79, 'PricewaterhouseCoopers LLP', 'One Kingsway, Cardiff', 'CF10 3PW', '029 2023 7000', '', '', ['Registered by the ICAEW to carry out audit work', ''], [], [u'Derek Howell']], [80, 'PricewaterhouseCoopers LLP', 'Waterfront Plaza, 8 Laganbank Road, Belfast', 'BT1 3LR', '028 9024 5454', '', '', ['Registered by the ICAEW to carry out audit work', ''], [], [u'Martin Cowie']]]\n"
     ]
    }
   ],
   "source": [
    "firms_matrix = []\n",
    "\n",
    "# the code above is somewhat responsive to the data in the website, but the code below is manual\n",
    "# the commented out code is currently inoperative because it relies on designatory data existing\n",
    "# that doesn't in the test data.\n",
    "\n",
    "for i in range(len(firms)):\n",
    "    firms_matrix.append([int(firms[i][0]),firms[i][1][0].encode('ascii','replace'),designatory_data[i][1],designatory_data[i][2],designatory_data[i][0],specialisations[i],company_profile[i],qualifications[i],staff_ICAEW[i],staff_other[i]])\n",
    "\n",
    "#    firms_matrix.append([int(firms[i][0]),firms[i][1][0].encode('ascii','replace'),designatory_data[i][1],designatory_data[i][2],designatory_data[i][0],designatory_data[i][3],designatory_data[i][4],specialisations[i],company_profile[i],qualifications[i],staff_ICAEW[i],staff_other[i]])\n",
    "\n",
    "firms_matrix.sort()\n",
    "print(firms_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
